version: "3.9"

services:
  rag-backend:
    build:
      context: ./api
      dockerfile: Dockerfile
    image: vectorpathconsulting/rag-chat-backend:latest
    container_name: rag-chat-backend
    ports:
      - "5001:5001"
    env_file:
      - ./api/.env
    environment:
      # Inference gateway base URL
      BASE_URL: ${BASE_URL:-https://api.example.com}

      # Embeddings endpoint base URL
      # Default is a remote OpenAI compatible API.
      # To use the local embeddings service, set:
      #   EMBEDDINGS_BASE_URL=http://local-embeddings:8080
      EMBEDDINGS_BASE_URL: ${EMBEDDINGS_BASE_URL:-https://api.example.com}

      # Model hints
      EMBEDDING_MODEL_NAME: ${EMBEDDING_MODEL_NAME:-bge-base-en-v1.5}
      INFERENCE_MODEL_NAME: ${INFERENCE_MODEL_NAME:-meta-llama/Llama-3.1-8B-Instruct}
      EMBEDDING_MODEL_ENDPOINT: ${EMBEDDING_MODEL_ENDPOINT:-bge-base-en-v1.5}
      INFERENCE_MODEL_ENDPOINT: ${INFERENCE_MODEL_ENDPOINT:-Llama-3.1-8B-Instruct}
    networks:
      - appnet
    restart: unless-stopped
    # If you want to enforce that embeddings are local and ready before startup,
    # you can uncomment this block:
    # depends_on:
    #   - local-embeddings

  rag-frontend:
    build:
      context: ./ui
      dockerfile: Dockerfile
    image: vectorpathconsulting/rag-chat-frontend:latest
    container_name: rag-chat-frontend
    depends_on:
      - rag-backend
    environment:
      # Browser calls /api; Vite proxy forwards to backend service
      VITE_API_URL: "/api"
      BACKEND_PROXY_URL: "http://rag-backend:5001"
    ports:
      - "8084:3000"
    networks:
      - appnet
    restart: unless-stopped

  # Optional local embeddings service
  # Built from ./embeddings_service which should expose /health and /v1/embeddings on port 8080
  local-embeddings:
    build:
      context: ./embeddings_service
      dockerfile: Dockerfile
    image: vectorpathconsulting/rag-local-embeddings:latest
    container_name: rag-local-embeddings
    environment:
      # Used by the simple FastAPI server inside embeddings_service
      MODEL_NAME: ${EMBEDDING_MODEL_NAME:-bge-base-en-v1.5}
    ports:
      # Expose on host 8088 for local curl testing:
      #   curl http://localhost:8088/health
      #   curl http://localhost:8088/v1/embeddings ...
      - "8088:8080"
    networks:
      - appnet
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  info:
    image: alpine:latest
    container_name: rag-chat-info
    command: >
      sh -c "
      echo '';
      echo '==============================================';
      echo '   Inference Blueprint: RAG Chatbot';
      echo '----------------------------------------------';
      echo '   API:      http://localhost:5001';
      echo '   Frontend: http://localhost:8084';
      echo '   Local embeddings (optional): http://localhost:8088';
      echo '==============================================';
      echo '';
      sleep 10;
      "
    networks:
      - appnet

networks:
  appnet:
    driver: bridge
