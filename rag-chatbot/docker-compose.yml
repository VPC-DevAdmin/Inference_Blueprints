version: "3.9"

services:
  rag-backend:
    build:
      context: ./api
      dockerfile: Dockerfile
    image: vectorpathconsulting/rag-chat-backend:latest
    container_name: rag-chat-backend
    ports:
      - "5001:5001"
    env_file:
      - ./api/.env
    environment:
      # Allow overriding model selections via root .env or runtime env
      EMBEDDING_MODEL_NAME: ${EMBEDDING_MODEL_NAME:-bge-base-en-v1.5}
      INFERENCE_MODEL_NAME: ${INFERENCE_MODEL_NAME:-meta-llama/Llama-3.1-8B-Instruct}
      EMBEDDING_MODEL_ENDPOINT: ${EMBEDDING_MODEL_ENDPOINT:-bge-base-en-v1.5}
      INFERENCE_MODEL_ENDPOINT: ${INFERENCE_MODEL_ENDPOINT:-Llama-3.1-8B-Instruct}
    networks:
      - appnet
    restart: unless-stopped

  rag-frontend:
    build:
      context: ./ui
      dockerfile: Dockerfile
    image: vectorpathconsulting/rag-chat-frontend:latest
    container_name: rag-chat-frontend
    depends_on:
      - rag-backend
    environment:
      # Browser calls /api; Vite proxy forwards to backend service
      VITE_API_URL: "/api"
      BACKEND_PROXY_URL: "http://rag-backend:5001"
    ports:
      - "8084:3000"
    networks:
      - appnet
    restart: unless-stopped

  info:
    image: alpine:latest
    container_name: rag-chat-info
    command: >
      sh -c "
      echo '';
      echo '==============================================';
      echo '   Inference Blueprint: RAG Chatbot';
      echo '----------------------------------------------';
      echo '   API:      http://localhost:5001';
      echo '   Frontend: http://localhost:8084';
      echo '==============================================';
      echo '';
      sleep 10;
      "
    networks:
      - appnet

networks:
  appnet:
    driver: bridge
